{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-OKxKvU3j3SP"
   },
   "source": [
    "# Double Deep Q-Learning to play 2048\n",
    "\n",
    "by Jennifer Bryson, Madina Abdrakhmanova, and Caleb Nelson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this group project we used the code outline given from chapter 16 in the O'Reilly textbook Hands On Machine Learning with Scikit-Learn and TensorFlow by Aurélien Géron.  In the textbook, they followed the ideas of DeepMind and used reinforcement learning to play Ms. Pac-Man.  We modified the code to play 2048 instead.  We also added \"double\" deep Q-learning to improve performance.  Our overall outcome is that we found several different neural network configurations which can get to the 512 tile around 5% of the time (which never happens for random gameplay) with as little as 35 minutes worth of training time, but more work must be done to reach 2048.  Here's what we have so far!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must make the 2048 game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from types import *\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "\n",
    "DirectionMap = { 0:'up',    'up':0,    'u':0,\n",
    "                 1:'right', 'right':1, 'r':1, \n",
    "                 2:'down',  'down':2,  'd':2,\n",
    "                 3:'left',  'left':3,  'l':3 }\n",
    "\n",
    "class Board:\n",
    "    board = []\n",
    "    score = 0\n",
    "\n",
    "    def __init__(self, grid = None, scr = None):\n",
    "        self.board = []\n",
    "        if (scr == None):\n",
    "            scr = 0\n",
    "        self.score = int(scr)\n",
    "        if (grid == None):\n",
    "            grid = \"2,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0\"\n",
    "        if (type(grid) is list):\n",
    "            self.board = copy.deepcopy(grid)\n",
    "        else:\n",
    "            grid = list(map(int, grid.replace(' ', '').split(',')))\n",
    "            if len(grid) < 16:\n",
    "                grid.extend([0 for x in range(16-len(grid))])\n",
    "            for row in range(0, 4):\n",
    "                self.board.append(grid[4*row:4*(row+1)])\n",
    "\n",
    "    def __str__(self):\n",
    "        rowStrs = []\n",
    "        for row in self.board:\n",
    "            rowStrs.append(','.join(map(str, row)))\n",
    "        return (', '.join(rowStrs) + \", \" + str(self.score))\n",
    "\n",
    "    #returns true if board[r1][c1] can move onto board[r0][c0], false otherwise\n",
    "    def moveOk(self, r0, c0, r1, c1):\n",
    "        a = self.board[r0][c0]\n",
    "        b = self.board[r1][c1]\n",
    "        return (a == 0 and b != 0) or (a != 0 and a == b)\n",
    "\n",
    "    #returns true if dir is a valid move for board, false otherwise\n",
    "    def tryDir(self, dir):\n",
    "        if(type(dir) == int):\n",
    "            dir = DirectionMap[dir]\n",
    "        if (dir == 'l' or dir == \"left\"):\n",
    "            for row in range(0,4):\n",
    "                for col in range(0, 3):\n",
    "                    if (self.moveOk(row, col, row, col+1)):\n",
    "                        return True\n",
    "        elif (dir == 'u' or dir == \"up\"):\n",
    "            for col in range(0, 4):\n",
    "                for row in range(0, 3):\n",
    "                    if (self.moveOk(row, col, row+1, col)):\n",
    "                        return True\n",
    "        elif (dir == 'r' or dir == \"right\"):\n",
    "            for row in range(0,4):\n",
    "                for col in range(1, 4):\n",
    "                    if (self.moveOk(row, col, row, col-1)):\n",
    "                        return True\n",
    "        elif (dir == 'd' or dir == \"down\"):\n",
    "            for col in range(0, 4):\n",
    "                for row in range(1, 4):\n",
    "                    if (self.moveOk(row, col, row-1, col)):\n",
    "                        return True\n",
    "        return False\n",
    "\n",
    "    #returns true if the board has any valid moves, false otherwise\n",
    "    def canMove(self):\n",
    "        return(self.tryDir('l') or self.tryDir('u') or self.tryDir('r') or self.tryDir('d'))\n",
    "\n",
    "    #returns a board that would be the result if the current board was moved in direction dir\n",
    "    def move(self, dir):\n",
    "        if(type(dir) == int):\n",
    "            dir = DirectionMap[dir]\n",
    "        newboard = Board(self.board, self.score)\n",
    "        newscr = self.score\n",
    "        if (not self.tryDir(dir)):\n",
    "            return newboard\n",
    "        else:\n",
    "            if (dir == 'l' or dir == \"left\"):\n",
    "                for row in range(0, 4):\n",
    "                    vscore = makeMove(newboard.board[row])\n",
    "                    newboard.board[row] = vscore[0]\n",
    "                    newscr += vscore[1]\n",
    "            elif (dir == 'u' or dir == \"up\"):\n",
    "                for col in range(0, 4):\n",
    "                    vector = []\n",
    "                    for row in range(0, 4):\n",
    "                        vector.append(newboard.board[row][col])\n",
    "                    vscore = makeMove(vector)\n",
    "                    newscr += vscore[1]\n",
    "                    for row in range(0, 4):\n",
    "                        newboard.board[row][col] = vscore[0][row]\n",
    "            elif (dir == 'r' or dir == \"right\"):\n",
    "                for row in range(0, 4):\n",
    "                    vector = []\n",
    "                    for col in range(0, 4):\n",
    "                        vector.append(newboard.board[row][3-col])\n",
    "                    vscore = makeMove(vector)\n",
    "                    newscr += vscore[1]\n",
    "                    for col in range(0, 4):\n",
    "                        newboard.board[row][3-col] = vscore[0][col]\n",
    "            elif (dir == 'd' or dir == \"down\"):\n",
    "                for col in range(0, 4):\n",
    "                    vector = []\n",
    "                    for row in range(0, 4):\n",
    "                        vector.append(newboard.board[3-row][col])\n",
    "                    vscore = makeMove(vector)\n",
    "                    newscr += vscore[1]\n",
    "                    for row in range(0, 4):\n",
    "                        newboard.board[3-row][col] = vscore[0][row]\n",
    "            newboard.score = newscr\n",
    "            return newboard\n",
    "\n",
    "    #returns true if the board can move num times, false if it can't.\n",
    "    def moveNum(self, num):\n",
    "        if (num <= 1):\n",
    "            return self.canMove()\n",
    "        return(self.move('l').drop().moveNum(num-1) or\n",
    "               self.move('u').drop().moveNum(num-1) or\n",
    "               self.move('r').drop().moveNum(num-1) or\n",
    "               self.move('d').drop().moveNum(num-1))\n",
    "\n",
    "\n",
    "    #returns an array of tuples that represent empty board locations, namely those that can recieve a random drop\n",
    "    def possibleDrops(self):\n",
    "        drops = []\n",
    "        for row in range(0, 4):\n",
    "            for col in range(0, 4):\n",
    "                if (self.board[row][col] == 0):\n",
    "                    drops.append((row, col))\n",
    "        return drops\n",
    "\n",
    "    #returns a board that would be the result if the current board were to recieve a random drop from list drops, or from possibleDrops if drops is null\n",
    "    def drop(self, drops = None):\n",
    "        if (drops == None):\n",
    "            drops = self.possibleDrops()\n",
    "        newboard = Board(self.board, self.score)\n",
    "        if (len(drops) == 0):\n",
    "            return newboard\n",
    "        choice = random.choice(drops)\n",
    "        if (random.random() >= .9):\n",
    "            newboard.board[choice[0]][choice[1]] = 4\n",
    "        else:\n",
    "            newboard.board[choice[0]][choice[1]] = 2\n",
    "        return newboard\n",
    "\n",
    "#the following are helper functions to make move work\n",
    "def squishZeros(vector):\n",
    "    index = 0\n",
    "    for i in range(0, len(vector)):\n",
    "        if (vector[i] != 0):\n",
    "            vector[index] = vector[i]\n",
    "            if (i != index):\n",
    "                vector[i] = 0\n",
    "            index += 1\n",
    "    return vector\n",
    "\n",
    "def makeFusions(vector):\n",
    "    newscr = 0\n",
    "    for i in range(0, len(vector)-1):\n",
    "        if (vector[i] == vector[i+1]):\n",
    "            vector[i] += vector[i+1]\n",
    "            newscr += vector[i]\n",
    "            vector[i+1] = 0\n",
    "    return [vector, newscr]\n",
    "\n",
    "def makeMove(vector):\n",
    "    vector = squishZeros(vector)\n",
    "    vscore = makeFusions(vector)\n",
    "    vscore[0] = squishZeros(vscore[0])\n",
    "    return vscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the packages we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "adeg8aIasn1V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** the next block of code is only for running on Colab.  Do not run it if you're running this on Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1521771566798,
     "user": {
      "displayName": "Madina Abdrakhmanova",
      "photoUrl": "//lh6.googleusercontent.com/-HiVhKehsFks/AAAAAAAAAAI/AAAAAAAABV4/qYrfNQrfbrQ/s50-c-k-no/photo.jpg",
      "userId": "115510462428638216517"
     },
     "user_tz": 420
    },
    "id": "BpD3TFPDyipp",
    "outputId": "81c7f2c5-173d-4484-91a9-67c408edb9f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is where the fun begins!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vw_LmzvTsn1f"
   },
   "source": [
    "## Q Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ysozQADwyTZ9"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we specify the convolutional neural network that we will use.  We've played around with several different configurations.  This one appears to be good, although we haven't done an exhuastive search, so more work could be done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "biJgYlsSsn1h"
   },
   "outputs": [],
   "source": [
    "input_height = 4\n",
    "input_width = 4\n",
    "input_channels = 1\n",
    "conv_n_maps = [128, 128]\n",
    "conv_kernel_sizes = [(2,2), (2,2)]\n",
    "conv_strides = [1, 1]\n",
    "conv_paddings = [\"VALID\"] * 2 \n",
    "conv_activation = [tf.nn.relu] * 2\n",
    "n_hidden_in = 128 * 2 * 2  # conv3 has 128 maps of 2x2 each\n",
    "n_hidden = 256\n",
    "hidden_activation = tf.nn.relu\n",
    "n_outputs = 4  # 4 discrete actions are available\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "def q_network(X_state, name):\n",
    "    prev_layer = X_state\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        for n_maps, kernel_size, strides, padding, activation in zip(\n",
    "                conv_n_maps, conv_kernel_sizes, conv_strides,\n",
    "                conv_paddings, conv_activation):\n",
    "            prev_layer = tf.layers.conv2d(\n",
    "                prev_layer, filters=n_maps, kernel_size=kernel_size,\n",
    "                strides=strides, padding=padding, activation=activation,\n",
    "                kernel_initializer=initializer)\n",
    "        last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])\n",
    "        hidden = tf.layers.dense(last_conv_layer_flat, n_hidden,\n",
    "                                 activation=hidden_activation,\n",
    "                                 kernel_initializer=initializer)\n",
    "        outputs = tf.layers.dense(hidden, n_outputs,\n",
    "                                  kernel_initializer=initializer)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                       scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name):]: var\n",
    "                              for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use two networks: online and target.  This is one of the \"tricks\" by DeepMind which increased stability and success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1QfZGsqmsn1j"
   },
   "outputs": [],
   "source": [
    "X_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width,\n",
    "                                            input_channels])\n",
    "online_q_values, online_vars = q_network(X_state, name=\"q_networks/online\")\n",
    "target_q_values, target_vars = q_network(X_state, name=\"q_networks/target\")\n",
    "\n",
    "copy_ops = [target_var.assign(online_vars[var_name])\n",
    "            for var_name, target_var in target_vars.items()]\n",
    "copy_online_to_target = tf.group(*copy_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1521771575697,
     "user": {
      "displayName": "Madina Abdrakhmanova",
      "photoUrl": "//lh6.googleusercontent.com/-HiVhKehsFks/AAAAAAAAAAI/AAAAAAAABV4/qYrfNQrfbrQ/s50-c-k-no/photo.jpg",
      "userId": "115510462428638216517"
     },
     "user_tz": 420
    },
    "id": "FK0ZLRL-sn1l",
    "outputId": "cfe94552-5dea-440d-a87c-6d19f7595db9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/conv2d/bias:0': <tf.Variable 'q_networks/online/conv2d/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " '/conv2d/kernel:0': <tf.Variable 'q_networks/online/conv2d/kernel:0' shape=(2, 2, 1, 128) dtype=float32_ref>,\n",
       " '/conv2d_1/bias:0': <tf.Variable 'q_networks/online/conv2d_1/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " '/conv2d_1/kernel:0': <tf.Variable 'q_networks/online/conv2d_1/kernel:0' shape=(2, 2, 128, 128) dtype=float32_ref>,\n",
       " '/dense/bias:0': <tf.Variable 'q_networks/online/dense/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " '/dense/kernel:0': <tf.Variable 'q_networks/online/dense/kernel:0' shape=(512, 256) dtype=float32_ref>,\n",
       " '/dense_1/bias:0': <tf.Variable 'q_networks/online/dense_1/bias:0' shape=(4,) dtype=float32_ref>,\n",
       " '/dense_1/kernel:0': <tf.Variable 'q_networks/online/dense_1/kernel:0' shape=(256, 4) dtype=float32_ref>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RMApjyVlsn1p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-86b805f1efd9>:8: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "momentum = 0.95\n",
    "\n",
    "with tf.variable_scope(\"train\"):\n",
    "    X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    q_value = tf.reduce_sum(online_q_values * tf.one_hot(X_action, n_outputs),\n",
    "                            axis=1, keep_dims=True)\n",
    "    error = tf.abs(y - q_value)\n",
    "    clipped_error = tf.clip_by_value(error, 0.0, 1.0)\n",
    "    linear_error = 2 * (error - clipped_error)\n",
    "    loss = tf.reduce_mean(tf.square(clipped_error) + linear_error)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another crucial trick is replay memory.  We don't want to be training on subsequent gameboard states because those are highly correlated and this could cause us to find a local minimum instead of a global minimum.  Instead, we store 25,000 game states (roughly 1/8 of the total number of training iterations) and we will randomly choose batch_size=50 of these states for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EWykgJU5sn1t"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory_size = 25000\n",
    "replay_memory = deque([], maxlen=replay_memory_size)\n",
    "\n",
    "def sample_memories(batch_size):\n",
    "    indices = np.random.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for idx in indices:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the exploration/exploitation tradeoff, we will start with exploring 100% of the time and exponentially decrease the exploration to 10% of the time over the course of 100,000 steps.  Note: the decay step length should be related to the total number of training steps.  From other works, it seems the decay step length should be roughly half of the total number of training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CUje-3ulsn1v"
   },
   "outputs": [],
   "source": [
    "eps_min = 0.1\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 100000\n",
    "\n",
    "def epsilon_greedy(q_values, step, state):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs) # random action\n",
    "    \n",
    "    else:\n",
    "        L = np.argsort(-q_values)\n",
    "        if state.tryDir(int(L[0][0])): #if the action with the highest q_value is a valid move,\n",
    "            return int(L[0][0])  #return the action with the highest q_value\n",
    "        elif state.tryDir(int(L[0][1])):\n",
    "            return int(L[0][1])  #return the action with the second highest q_value\n",
    "        elif state.tryDir(int(L[0][2])):\n",
    "            return int(L[0][2])  #return the action with the third highest q_value\n",
    "        else:\n",
    "            return int(L[0][3])  #return the action with the lowest q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the total number of training steps, the batch_size for replay memory, the discount_rate, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TCu2sPg_sn1y"
   },
   "outputs": [],
   "source": [
    "n_steps = 200000  # total number of training steps\n",
    "training_start = 10000  # start training after 10,000 game iterations\n",
    "training_interval = 4  # run a training step every 4 game iterations\n",
    "save_steps = 1000  # save the model every 1,000 training steps\n",
    "copy_steps = 10000  # copy online DQN to target DQN every 10,000 training steps\n",
    "discount_rate = 0.99\n",
    "skip_start = 90  # Skip the start of every game (it's just waiting time).\n",
    "batch_size = 50\n",
    "\n",
    "iteration = 0  # game iterations\n",
    "checkpoint_path = \"./my_dqn.ckpt\"\n",
    "done = True # env needs to be reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cVYXUMHPsn11"
   },
   "outputs": [],
   "source": [
    "loss_val = np.infty\n",
    "game_length = 0\n",
    "total_max_q = 0\n",
    "mean_max_q = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_11AbtMusn15"
   },
   "source": [
    "This next block of code actually runs the training.  Warning: it will take around 35 minutes!\n",
    "\n",
    "For claification, below \"done\" takes on the value 0 or 1 via done = 1 if the game is over, and done = 0 if it's not.\n",
    "This is not to be confused with \"continue\" used in the replay memory.  In the replay memory, we store (state, action, reward, next_state, continue) where continue is the opposite of done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1102,
     "output_extras": [
      {
       "item_id": 11
      },
      {
       "item_id": 12
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3053,
     "status": "error",
     "timestamp": 1521771603969,
     "user": {
      "displayName": "Madina Abdrakhmanova",
      "photoUrl": "//lh6.googleusercontent.com/-HiVhKehsFks/AAAAAAAAAAI/AAAAAAAABV4/qYrfNQrfbrQ/s50-c-k-no/photo.jpg",
      "userId": "115510462428638216517"
     },
     "user_tz": 420
    },
    "id": "zirUd86vsn18",
    "outputId": "e4326d75-4544-4d84-a3e5-6b9fa425cc9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "Iteration 813992\tTraining step 199999/200000 (100.0)%\tLoss 20.804014\tMean Max-Q 139.137575   "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path + \".index\"):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "        copy_online_to_target.run()\n",
    "    while True:\n",
    "        step = global_step.eval()  \n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        print(\"\\rIteration {}\\tTraining step {}/{} ({:.1f})%\\tLoss {:5f}\\tMean Max-Q {:5f}   \".format(\n",
    "            iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q), end=\"\")\n",
    "        if done: # game over, start again\n",
    "            obs = Board()\n",
    "            #for skip in range(skip_start): # skip the start of each game\n",
    "            #    obs, reward, done, info = env.step(0)\n",
    "            #state = preprocess_observation(obs)\n",
    "            state = obs\n",
    "\n",
    "        # Online DQN evaluates what to do\n",
    "        temp = np.array(state.board).reshape(4,4,1)\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [temp]})\n",
    "        #print('online q values', q_values)\n",
    "        action = epsilon_greedy(q_values, step, state)\n",
    "\n",
    "        # Online DQN plays\n",
    "        #next_state, reward, done, info= env.step(action)\n",
    "        if (state.tryDir(action)):\n",
    "            next_state = state.move(int(action)).drop()\n",
    "            reward = next_state.score - state.score\n",
    "        else:\n",
    "            next_state = state\n",
    "            reward = -100 #penalty for wasting our time\n",
    "        done = (0 if next_state.canMove() else 1)\n",
    "        \n",
    "        # Let's memorize what happened\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "\n",
    "        # Compute statistics for tracking progress (not shown in the book)\n",
    "        total_max_q += q_values.max()\n",
    "        game_length += 1\n",
    "        if done:\n",
    "            mean_max_q = total_max_q / game_length\n",
    "            total_max_q = 0.0\n",
    "            game_length = 0\n",
    "\n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue # only train after warmup period and at regular intervals\n",
    "        \n",
    "        # Sample memories and use the target DQN to produce the target Q-Value\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "            sample_memories(batch_size))\n",
    "        \n",
    "        X_next_state_val_board = []\n",
    "        for state in X_next_state_val:\n",
    "            X_next_state_val_board.append(np.array(state.board).reshape(4,4,1))\n",
    "        \n",
    "        #Standart Q learning\n",
    "        #next_q_values = target_q_values.eval(\n",
    "        #    feed_dict={X_state: X_next_state_val_board})\n",
    "\n",
    "        #max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        #y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "        \n",
    "        #Double Q Learning\n",
    "        target_next_q_values = target_q_values.eval(\n",
    "            feed_dict={X_state: X_next_state_val_board})\n",
    "        online_next_q_values = online_q_values.eval(\n",
    "            feed_dict={X_state: X_next_state_val_board})\n",
    "        next_actions = np.argmax(online_next_q_values, axis=1)\n",
    "        chosen_next_q_values = target_next_q_values[np.arange(len(next_actions)), next_actions].reshape(batch_size,1)\n",
    "        y_val = rewards + continues * discount_rate * chosen_next_q_values\n",
    "\n",
    "        \n",
    "        # Train the online DQN\n",
    "        #print('X_state_val shape', X_state_val.shape)\n",
    "        X_state_val_board = []\n",
    "        for state in X_state_val:\n",
    "            X_state_val_board.append(np.array(state.board).reshape(4,4,1))\n",
    "        \n",
    "        _, loss_val = sess.run([training_op, loss], feed_dict={\n",
    "            X_state: X_state_val_board, X_action: X_action_val, y: y_val})\n",
    "\n",
    "        # Regularly copy the online DQN to the target DQN\n",
    "        if step % copy_steps == 0:\n",
    "            copy_online_to_target.run()\n",
    "\n",
    "        # And save regularly\n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it's been trained!  Now to see how it does!  This next block of code plays the game with the learned network 100 different times and saves the final score and the final gameboard frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 20417,
     "output_extras": [
      {
       "item_id": 28
      },
      {
       "item_id": 58
      },
      {
       "item_id": 91
      },
      {
       "item_id": 130
      },
      {
       "item_id": 171
      },
      {
       "item_id": 211
      },
      {
       "item_id": 249
      },
      {
       "item_id": 280
      },
      {
       "item_id": 306
      },
      {
       "item_id": 331
      },
      {
       "item_id": 370
      },
      {
       "item_id": 407
      },
      {
       "item_id": 446
      },
      {
       "item_id": 486
      },
      {
       "item_id": 520
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 147934,
     "status": "ok",
     "timestamp": 1521766517528,
     "user": {
      "displayName": "Madina Abdrakhmanova",
      "photoUrl": "//lh6.googleusercontent.com/-HiVhKehsFks/AAAAAAAAAAI/AAAAAAAABV4/qYrfNQrfbrQ/s50-c-k-no/photo.jpg",
      "userId": "115510462428638216517"
     },
     "user_tz": 420
    },
    "id": "SYx6MxGZsn2B",
    "outputId": "cdbb5f66-1827-465c-e720-20b3f4413fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial:  0\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  1\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  2\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  3\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  4\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  5\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  6\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  7\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  8\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  9\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  10\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  11\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  12\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  13\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  14\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  15\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  16\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  17\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  18\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  19\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  20\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  21\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  22\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  23\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  24\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  25\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  26\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  27\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  28\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  29\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  30\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  31\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  32\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  33\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  34\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  35\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  36\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  37\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  38\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  39\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  40\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  41\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  42\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  43\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  44\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  45\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  46\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  47\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  48\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  49\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  50\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  51\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  52\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  53\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  54\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  55\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  56\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  57\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  58\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  59\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  60\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  61\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  62\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  63\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  64\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  65\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  66\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  67\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  68\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  69\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  70\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  71\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  72\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  73\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  74\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  75\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  76\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  77\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  78\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  79\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  80\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  81\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  82\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  83\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  84\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  85\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  86\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  87\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  88\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  89\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  90\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  91\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  92\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  93\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  94\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  95\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  96\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  97\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  98\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "trial:  99\n",
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_trials = 100\n",
    "final_frames = []\n",
    "final_scores = np.zeros(num_trials)\n",
    "for i in range (num_trials):\n",
    "    print('trial: ', i)\n",
    "    #frames = []\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "\n",
    "        step_count = 0\n",
    "\n",
    "        obs = Board()\n",
    "\n",
    "        while(True):\n",
    "            state = obs\n",
    "            state_board = np.array(state.board).reshape(4,4,1)\n",
    "            # Online DQN evaluates what to do\n",
    "            q_values = online_q_values.eval(feed_dict={X_state: [state_board]})\n",
    "            #print(q_values)\n",
    "\n",
    "            L = np.argsort(-q_values)\n",
    "            #print(L)\n",
    "            if state.tryDir(int(L[0][0])): #if the action with the highest q_value is a valid move,\n",
    "                action = L[0][0]  #return the action with the highest q_value\n",
    "            elif state.tryDir(int(L[0][1])):\n",
    "                action = L[0][1]  #return the action with the second highest q_value\n",
    "            elif state.tryDir(int(L[0][2])):\n",
    "                action = L[0][2]  #return the action with the third highest q_value\n",
    "            else:\n",
    "                action = L[0][3]  #return the action with the lowest q_value\n",
    "\n",
    "\n",
    "            #action = np.argmax(q_values)\n",
    "            #print(action)\n",
    "            # Online DQN plays\n",
    "            next_state = state.move(int(action)).drop()\n",
    "\n",
    "            #print(next_state.board)\n",
    "            reward = next_state.score - state.score\n",
    "            done = (0 if next_state.canMove() else 1)\n",
    "\n",
    "            obs = next_state\n",
    "\n",
    "            img = obs.board\n",
    "            #frames.append(img)\n",
    "\n",
    "            step_count = step_count+1\n",
    "\n",
    "            if done:\n",
    "                #print('Game Over, you lasted', step_count, 'steps.')\n",
    "                #print('Final Score: ', obs.score)\n",
    "                final_scores[i] = obs.score\n",
    "                final_frames.append(img)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the average final score to see roughly how well it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Vizz9xLKqw4-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1784.6\n",
      "913.6733496709805\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(final_scores))\n",
    "print(np.std(final_scores, ddof = 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can print out the log base 2 of the largest tile seen for each game (meaning if it got to 512 it would print 9 since $2^9$=512)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "i6d_fXTHqxnj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[7, 6, 6, 7, 8, 7, 6, 7, 8, 7, 8, 7, 7, 8, 8, 8, 8, 5, 7, 8, 8, 8, 8, 6, 8, 7, 8, 6, 6, 7, 6, 6, 7, 7, 6, 7, 6, 8, 8, 8, 7, 6, 7, 6, 7, 8, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 5, 9, 7, 5, 6, 7, 6, 7, 7, 7, 6, 7, 7, 6, 5, 7, 7, 7, 6, 6, 7, 6, 7, 7, 7, 7, 5, 7, 8, 6, 7, 8, 7, 6, 8, 6]\n"
     ]
    }
   ],
   "source": [
    "max_tiles = []\n",
    "curr_max_tiles= []\n",
    "print(len(final_frames))\n",
    "for curr_final_frame in final_frames:\n",
    "    max_tile = np.amax(curr_final_frame)\n",
    "    log2_max_tile = int(np.log2(max_tile))\n",
    "    curr_max_tiles.append(log2_max_tile)\n",
    "print (curr_max_tiles)\n",
    "max_tiles.append(curr_max_tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our highest tile from these 100 gameplays was 512, which is better than random gameplay so we've definitely learned something good!  Training for many more iterations will definitely improve this further, as will tinkering with parameters and neural network configurations some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "2048_DDQN_128_2conv_layers.ipynb",
   "provenance": [
    {
     "file_id": "1elUyiMsbs3UD-bf1lS9hNXQGE4PwdDaH",
     "timestamp": 1521606947332
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
